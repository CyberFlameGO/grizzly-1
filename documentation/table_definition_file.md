# Table Definition File
Table definition files provide instructions to Grizzly on how to generate your BigQuery Tables. One table definition file exists for each table in your Grizzly installation. 

The files are written in YAML. The files should be named "[dataset].[table_name].yml"

For a table to be generated and updated, it needs to be listed in a [Scope file](./scope_file.md)'s etl_scope field. After creating a table definition file, be sure to add it to a [Scope file](./scope_file.md). 

| Field name | Notes |
|------------|-------|
| target_table_name  | Required. YAML Scalar. <br><br> The value should be “[target_dataset].[target_table]” (without the quotes.) |
| stage_loading_query | Optional for Google Sheets imports. Otherwise, Required. YAML Scalar. <br><br>Defines the relative Git path to the sql statement that will be executed to generate this table. <br><br> The Value should be “queries/[target_dataset].[target_table].sql” (without the quotes.) The .sql file name should be identical to the related Table Definition File name except for the suffix.
| job_write_mode | Required. YAML Scalar. <br><br>Specifies how data should be loaded into the BigQuery target table.<br><br> Allowed Values: <ul><li>WRITE_TRUNCATE: If the table already exists, overwrites the table data and uses the schema from the query result. </li><li>WRITE_APPEND: If the table already exists, append the data to the table.</li> <li>WRITE_EMPTY: If the table already exists and contains data, return a 'duplicate' error. </li> <li>UPDATE</li> <li>DELETE</li> </ul> |
| job_data_quality_query | Required. YAML Sequence. <br><br> Defines a list of relative Git paths to sql statements that will be executed as a business/operations or data logic check. <br><br> Value should be “queries/[quality_rule].sql” (without the quotes.) |
| schedule_interval | Optional. YAML Scalar. <br><br> This value should be specified if the ETL job for this table should be executed less frequently than the schedule defined for the domain by its “SCOPE.yml” file. The value is written using the CRON format. <br><br> If this field is not populated, the ETL job will run using the “SCOPE.yml” file’s schedule. |
| target_hx_loading_indicator | Optional. YAML Scalar. <br><br> It is a best practice to allow _hx tables only for the Production Presentation tables with a partition elimination value of 180 days. (May need some exceptions for unique ETL jobs that are building history.) Important: if the DDL of a _hx table is changing, be sure to migrate its data into the new structure before dropping the previous version of the table. <bR><br> Allowed Values: <ul><li>Y</li> <li>N: Default.</li></ul> |
| use_legacy_sql | Optional. YAML Scalar. <br><br>Allowed Values:<ul><li>Y</li><li>N: Default.</li></ul> |
| source_type | Optional. YAML Scalar. <bR><br> Allowed Values: <ul><li>bq: Default. For a single-step ETL job where the single job will populate or update the target table.</li><li>bq_scripting: For a multi-step ETL job where multiple, coordinated jobs will populate or update the same target table.</li> <li>gsheet: Data will be imported from a Google Sheets file. </li></ul>|
| source_gsheet_id | Optional. YAML Scalar. <br><br>When importing data from Google Sheets, value should be “"[gsheet_id]/'[gsheet_tab]'"” (without the outermost quotes.) If the Google Sheet contains only one tab, the value should be “"[gsheet_id]”” (without the outermost quotes.) |
| source_columns | Optional. YAML Sequence. <br><br>Defines a list of the specific [source_column]s to load and if each column should be force loaded as a string. <br><br>If this field is empty, the job will load every [source_column]. <br><br> Values are “- {source_name: "[source_column]", target_name: "[target_column]", force_string: Y or N}” (without the outermost quotes.) The values should be sorted alphabetically, indented by two spaces (no tabs), and aligned vertically. |
| trigger_rule | Optional. YAML Scalar.<br><br>Allowed Values: <ul><li>all_success</li><li>all_failed</li><li>all_done</li><li>one_failed</li><li>one_success</li><li>none_failed</li><li>none_skipped</li> </ul> |
| data_catalog_tags | Optional. YAML Sequence.<br><br>Defines the taxonomy tags for the [target_table] or a subset of its [target_column]s. <br><br>Value should be “datacatalog/[target_dataset].[target_table].json” (without the quotes.) <br><br>These values will only be applied if the “job_write_mode” field has a value of “WRITE_TRUNCATE”, “WRITE_APPEND”, or “WRITE_EMPTY”. Follow the steps listed in the Creating and modifying - BigQuery taxonomy section of this document to build and maintain these tags. |
| column_policy_tags | Optional. YAML Sequence. <br><br> Defines the column-level security access policy on the [target_table] using the taxonomy tags specified by the data_catalog_tags field. <br><br>Values should be “- [target_column]: "[column_policy]"” (without the outermost quotes.) The values should be sorted alphabetically, indented by two spaces (no tabs), and aligned vertically. <br><br>These values will only be applied if the “job_write_mode” field has a value of “WRITE_TRUNCATE”, “WRITE_APPEND”, or “WRITE_EMPTY”. Follow the steps listed in the Creating and modifying - BigQuery column-level security section of this document to build and maintain these tags. |
| access_scripts | Optional. YAML Sequence. <br><br> Defines the row-level security access policy on the [target_table] using the specified access queries. <br><br> Values should be “- queries/security/row_access.[target_dataset].[target_table].sql” (without the quotes.) The values should be sorted alphabetically, indented by two spaces (no tabs), and aligned vertically. <br><bR> These values will only be applied if the “job_write_mode” field has a value of “WRITE_TRUNCATE”, “WRITE_APPEND”, or “WRITE_EMPTY”. Follow the steps listed in the Creating and modifying - BigQuery row-level security section of this document to build and maintain these scripts. |

